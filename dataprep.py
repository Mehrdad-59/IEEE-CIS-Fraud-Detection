# -*- coding: utf-8 -*-
"""IEEE-CIS Fraud Detection_DataPrep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PWzW8D99cbGIL4XesEzgsU49jGq6FucZ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import gc

import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import LabelEncoder

! gdown 1XOG7Z82IDUBHaJwKxDee1la6uNJ3oHTU

! gdown 1OngX7ISBeZguhlGnOlbAH1FVk9rz75N5

! gdown 153Sx97Jwg5gj9POQr3Gz8PjcjFviNjvY

! gdown 1C0dvs_LlSCAV2gU73jCru1OddlAChqaD

train_tr=pd.read_csv('train_transaction.csv')
train_id=pd.read_csv('train_identity.csv')
test_tr=pd.read_csv('test_transaction.csv')
test_id=pd.read_csv('test_identity.csv')

train=train_tr.merge(train_id, on='TransactionID', how='left')
test=test_tr.merge(test_id, on='TransactionID', how='left')

del train_tr,train_id,test_tr,test_id

"""**Reducing Data Sets' memory Size**"""

for col in train.columns:
    if train[col].dtype=='float64': train[col] = train[col].astype('float32')
    if train[col].dtype=='int64': train[col] = train[col].astype('int32')

for col in test.columns:
    if test[col].dtype=='float64': test[col] = test[col].astype('float32')
    if test[col].dtype=='int64': test[col] = test[col].astype('int32')

"""**Correcting Test Set's id features**"""

ids_=['id-01','id-02','id-03','id-04','id-05','id-06','id-07','id-08','id-09','id-10','id-11']
ids__=['id_01','id_02','id_03','id_04','id_05','id_06','id_07','id_08','id_09','id_10','id_11']
id=[]
for i in range(12,39):
  id.append('id_{}'.format(i))

ids=ids__+id

id2=[]
for i in range(12,39):
  id2.append('id-{}'.format(i))

ids2=ids_+id2

column_names=dict(zip(ids2,ids))
test=test.rename(column_names, axis=1)

"""**Creating Time features**"""

train['transaction_day']=np.floor(train['TransactionDT']/86400)
test['transaction_day']=np.floor(test['TransactionDT']/86400)

train['transaction_Year']=np.ceil(train['transaction_day']/365)
test['transaction_Year']=np.ceil(test['transaction_day']/365)

train['transaction_DayOfYear']=train['transaction_day'].apply(lambda x:x-365 if x>365 else x)
test['transaction_DayOfYear']=test['transaction_day'].apply(lambda x:x-365 if x>365 else x)

train['transaction_week']=np.ceil(train['transaction_day']/7)
test['transaction_week']=np.ceil(test['transaction_day']/7)

train['transaction_week']=train['transaction_week'].apply(lambda x:x-52 if x>52 else x)
test['transaction_week']=test['transaction_week'].apply(lambda x:x-52 if x>52  else x)

train['transaction_weekDay']=train['transaction_day'].apply(lambda x: 7 if x%7==0 else x%7 )
test['transaction_weekDay']=test['transaction_day'].apply(lambda x: 7 if x%7==0 else x%7 )

train['transaction_hour']=train['TransactionDT']%(3600*24)/3600//1
test['transaction_hour']=test['TransactionDT']%(3600*24)/3600//1

train.drop('TransactionDT', axis=1, inplace=True)
test.drop('TransactionDT', axis=1, inplace=True)

"""**Transaction Amount dervied features**"""

train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)
test['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)

train['TransactionAmt_decimal_lenght'] = train['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()
test['TransactionAmt_decimal_lenght'] = test['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()

df=pd.concat([train, test])
df['TransactionAmt_Outlier']=np.abs(df.TransactionAmt-train.TransactionAmt.mean()) > (3*train.TransactionAmt.std())

train=df.iloc[:len(train)]
test=df.iloc[len(train):].drop('isFraud', axis=1).reset_index(drop=True)

del df
gc.collect()

"""**OS features**"""

train[['OS', 'OS_Version']]=train['id_30'].str.rsplit(n=1,expand=True)
test[['OS', 'OS_Version']]=test['id_30'].str.rsplit(n=1,expand=True)

train.drop('id_30', axis=1, inplace=True)
test.drop('id_30', axis=1, inplace=True)

train.drop('OS_Version', axis=1, inplace=True)
test.drop('OS_Version', axis=1, inplace=True)

"""**billing location**"""

df=pd.concat([train,test])
df['billing_location']=df['addr2'].astype('str')+'_'+df['addr1'].astype('str')

train=df.iloc[:len(train)]
test=df.iloc[len(train):].drop('isFraud', axis=1).reset_index(drop=True)

del df
gc.collect()

"""**IP PROXY**"""

df=pd.concat([train,test])
df['IP_PROXY'] = df['id_23'].str.split(':', expand=True)[1]

df.drop('id_23', axis=1, inplace=True)
train=df.iloc[:len(train)]
test=df.iloc[len(train):].drop('isFraud', axis=1).reset_index(drop=True)

del df
gc.collect()

"""**Device Name**"""

df=pd.concat([train,test])
df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]
df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'
df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'
df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'
df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'
df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'
df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'
df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'
df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'
df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'
df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'
df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'
df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'
df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'
df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'
df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'
df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'
df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'
df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = "Others"

df.drop('DeviceInfo', axis=1, inplace=True)
train=df.iloc[:len(train)]
test=df.iloc[len(train):].drop('isFraud', axis=1).reset_index(drop=True)

del df
gc.collect()

"""**Extracting Browser and Browser Version**"""

import re
train['Browser_version']=train['id_31'].astype('str').apply(lambda x: re.findall(r"[-+]?\d*\.\d+|\d+", x))
train['Browser_version']=train['Browser_version'].map(lambda x: np.nan if len(x) == 0 else x[0])

test['Browser_version']=test['id_31'].astype('str').apply(lambda x: re.findall(r"[-+]?\d*\.\d+|\d+", x))
test['Browser_version']=test['Browser_version'].map(lambda x: np.nan if len(x) == 0 else x[0])

train['Browser']=train['id_31'].str.replace('[-+]?\d*\.\d+|\d+', '')
train['Browser']=train['Browser'].str.rstrip()

test['Browser']=test['id_31'].str.replace('[-+]?\d*\.\d+|\d+', '')
test['Browser']=test['Browser'].str.rstrip()

train.drop('id_31', axis=1, inplace=True)
test.drop('id_31', axis=1, inplace=True)

df=pd.concat([train,test])
df.loc[df['Browser'].str.contains('chrome', na=False), 'Browser'] = 'Chrome'
df.loc[df['Browser'].str.contains('firefox', na=False), 'Browser'] = 'Firefox'
df.loc[df['Browser'].str.contains('safari', na=False), 'Browser'] = 'Safari'
df.loc[df['Browser'].str.contains('edge', na=False), 'Browser'] = 'Edge'
df.loc[df['Browser'].str.contains('ie', na=False), 'Browser'] = 'IE'
df.loc[df['Browser'].str.contains('samsung', na=False), 'Browser'] = 'Samsung'
df.loc[df['Browser'].str.contains('opera', na=False), 'Browser'] = 'Opera'
df['Browser'].fillna("NAN", inplace=True)
df.loc[df.Browser.isin(df.Browser.value_counts()[df.Browser.value_counts() < 200].index), 'Browser'] = "Others"

train=df.iloc[:len(train)]
test=df.iloc[len(train):].drop('isFraud', axis=1).reset_index(drop=True)

del df
gc.collect()

"""**id_34 / Match Status**"""

df=pd.concat([train, test])
df['match_status']=df['id_34'].str.split(':', n=1, expand=True)[1]

train=df.iloc[:len(train)]
test=df.iloc[len(train):].drop('isFraud', axis=1).reset_index(drop=True)

train.drop('id_34', axis=1, inplace=True)
test.drop('id_34', axis=1, inplace=True)

del df
gc.collect()

"""**email domain into bins**"""

emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 
          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',
          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',
          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 
          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',
          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',
          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 
          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 
          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',
          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',
          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',
          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 
          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 
          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 
          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 
          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 
          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 
          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',
          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}

for domain in ['P_emaildomain', 'R_emaildomain']:
    train[domain + '_bin'] = train[domain].map(emails)
    test[domain + '_bin'] = test[domain].map(emails)
    train[domain + '_suffix'] = train[domain].map(lambda x: str(x).split('.')[-1])
    test[domain + '_suffix'] = test[domain].map(lambda x: str(x).split('.')[-1])

"""**Frequency Encoding to find rare ocassions**"""

df=pd.concat([train,test])

card1_count = df['card1'].value_counts(dropna=True, normalize=True).to_dict()
card2_count = df['card2'].value_counts(dropna=True, normalize=True).to_dict()
card3_count = df['card3'].value_counts(dropna=True, normalize=True).to_dict()
card4_count = df['card4'].value_counts(dropna=True, normalize=True).to_dict()
card5_count = df['card5'].value_counts(dropna=True, normalize=True).to_dict()

location_count=df['billing_location'].value_counts(dropna=True, normalize=True).to_dict()
addr2_count=df['addr2'].value_counts(dropna=True, normalize=True).to_dict()
ProductCD_count=df['ProductCD'].value_counts(dropna=True, normalize=True).to_dict()
P_email_count=df['P_emaildomain'].value_counts(dropna=True, normalize=True).to_dict()
R_email_count=df['R_emaildomain'].value_counts(dropna=True, normalize=True).to_dict()
Tr_Hour_count=df['transaction_hour'].value_counts(dropna=True, normalize=True).to_dict()
Tr_weekday_count=df['transaction_weekDay'].value_counts(dropna=True, normalize=True).to_dict()

TimeZone_count=df['id_14'].value_counts(dropna=True, normalize=True).to_dict()
device_type_count=df['DeviceType'].value_counts(dropna=True, normalize=True).to_dict()
mobile_OS_count=df[df['DeviceType']=='mobile']['OS'].value_counts(dropna=True, normalize=True).to_dict()
Desktop_OS_count=df[df['DeviceType']=='desktop']['OS'].value_counts(dropna=True, normalize=True).to_dict()
mobile_Browser_count=df[df['DeviceType']=='mobile']['Browser'].value_counts(dropna=True, normalize=True).to_dict()
Desktop_Browser_count=df[df['DeviceType']=='desktop']['Browser'].value_counts(dropna=True, normalize=True).to_dict()
device_name_count=df['device_name'].value_counts(dropna=True, normalize=True).to_dict()
Decimal_length_count=df['TransactionAmt_decimal_lenght'].value_counts(dropna=True, normalize=True).to_dict()


train['card1_counts'] = train['card1'].map(card1_count)
train['card2_counts'] = train['card2'].map(card2_count)
train['card3_counts'] = train['card3'].map(card3_count)
train['card4_counts'] = train['card4'].map(card4_count)
train['card5_counts'] = train['card5'].map(card5_count)

train['billing_location_counts'] = train['billing_location'].map(location_count)
train['addr2_counts'] = train['addr2'].map(location_count)
train['ProductCD_counts'] = train['ProductCD'].map(ProductCD_count)
train['P_emaildomain_counts'] = train['P_emaildomain'].map(P_email_count)
train['R_emaildomain_counts'] = train['R_emaildomain'].map(R_email_count)
train['transaction_hour_counts'] = train['transaction_hour'].map(Tr_Hour_count)
train['transaction_weekDay_counts'] = train['transaction_weekDay'].map(Tr_weekday_count)
train['TimeZone_counts'] = train['id_14'].map(TimeZone_count)
train['DeviceType_counts'] = train['DeviceType'].map(device_type_count)
train['mobile_OS_counts'] = train[train['DeviceType']=='mobile']['OS'].map(mobile_OS_count)
train['Desktop_OS_counts'] = train[train['DeviceType']=='desktop']['OS'].map(Desktop_OS_count)
train['mobile_Browser_counts'] = train[train['DeviceType']=='mobile']['Browser'].map(mobile_Browser_count)
train['Desktop_Browser_counts'] = train[train['DeviceType']=='desktop']['Browser'].map(Desktop_Browser_count)
train['Decimal_length_counts'] = train['TransactionAmt_decimal_lenght'].map(Decimal_length_count)
train['device_name_counts'] = train['device_name'].map(device_name_count)

test['card1_counts'] = test['card1'].map(card1_count)
test['card2_counts'] = test['card2'].map(card2_count)
test['card3_counts'] = test['card3'].map(card3_count)
test['card4_counts'] = test['card4'].map(card4_count)
test['card5_counts'] = test['card5'].map(card5_count)

test['billing_location_counts'] = test['billing_location'].map(location_count)
test['addr2_counts'] = test['addr2'].map(location_count)
test['ProductCD_counts'] = test['ProductCD'].map(ProductCD_count)
test['P_emaildomain_counts'] = test['P_emaildomain'].map(P_email_count)
test['R_emaildomain_counts'] = test['R_emaildomain'].map(R_email_count)
test['transaction_hour_counts'] = test['transaction_hour'].map(Tr_Hour_count)
test['transaction_weekDay_counts'] = test['transaction_weekDay'].map(Tr_weekday_count)
test['TimeZone_counts'] = test['id_14'].map(TimeZone_count)
test['DeviceType_counts'] = test['DeviceType'].map(device_type_count)
test['mobile_OS_counts'] = test[test['DeviceType']=='mobile']['OS'].map(mobile_OS_count)
test['Desktop_OS_counts'] = test[test['DeviceType']=='desktop']['OS'].map(Desktop_OS_count)
test['mobile_Browser_counts'] = test[test['DeviceType']=='mobile']['Browser'].map(mobile_Browser_count)
test['Desktop_Browser_counts'] = test[test['DeviceType']=='desktop']['Browser'].map(Desktop_Browser_count)
test['Decimal_length_counts'] = test['TransactionAmt_decimal_lenght'].map(Decimal_length_count)
test['device_name_counts'] = test['device_name'].map(device_name_count)

del card1_count,card2_count,card3_count,card4_count,card5_count,location_count,ProductCD_count,P_email_count,R_email_count,device_type_count,mobile_OS_count,Desktop_OS_count,Tr_Hour_count,TimeZone_count,Tr_weekday_count
gc.collect()

temp=df.groupby('card1')['TransactionAmt_decimal_lenght'].value_counts(dropna=True, normalize=True).to_frame().rename({'TransactionAmt_decimal_lenght':'Count_tr_decimal_lenght_card1'},axis=1).reset_index()

train.merge(temp, on=['card1','TransactionAmt_decimal_lenght'], how='left')
test.merge(temp, on=['card1','TransactionAmt_decimal_lenght'], how='left')

del temp
gc.collect()

temp=df.groupby('addr2')['TransactionAmt_decimal_lenght'].value_counts(dropna=True, normalize=True).to_frame().rename({'TransactionAmt_decimal_lenght':'Count_tr_decimal_lenght_addr2'},axis=1).reset_index()

train.merge(temp, on=['addr2','TransactionAmt_decimal_lenght'], how='left')
test.merge(temp, on=['addr2','TransactionAmt_decimal_lenght'], how='left')

del temp
gc.collect()

del df
gc.collect()

"""**Some Grouped means to find abnormal Transactions**"""

df=pd.concat([train,test])
transaction_mean=train['TransactionAmt'].mean()
df['diff_tranAmt_mean']=df['TransactionAmt']-transaction_mean

train=df.iloc[:len(train)]
test=df.iloc[len(train):].drop('isFraud', axis=1).reset_index(drop=True)

columns_a = ['TransactionAmt', 'id_02', 'D15']
columns_b = ['card1', 'card4', 'addr1']

for col_a in columns_a:
    for col_b in columns_b:
        for df in [train, test]:
            df[f'{col_a}_to_mean_{col_b}'] = df[col_a] / df.groupby([col_b])[col_a].transform('mean')
            df[f'{col_a}_to_std_{col_b}'] = df[col_a] / df.groupby([col_b])[col_a].transform('std')

temp=df.groupby('transaction_week')['TransactionAmt'].agg('mean').to_frame().reset_index().rename({'TransactionAmt':'mean_weekly_TrAmt'}, axis=1)
train=train.merge(temp, on='transaction_week', how='left')
test=test.merge(temp, on='transaction_week', how='left')

train['diff_mean_weekly_TrAmt']=train['TransactionAmt']-train['mean_weekly_TrAmt']
test['diff_mean_weekly_TrAmt']=test['TransactionAmt']-test['mean_weekly_TrAmt']

del temp
gc.collect()

temp=df.groupby('transaction_day')['TransactionAmt'].agg('mean').to_frame().reset_index().rename({'TransactionAmt':'mean_daily_TrAmt'}, axis=1)
train=train.merge(temp, on='transaction_day', how='left')
test=test.merge(temp, on='transaction_day', how='left')

train['diff_mean_daily_TrAmt']=train['TransactionAmt']-train['mean_daily_TrAmt']
test['diff_mean_daily_TrAmt']=test['TransactionAmt']-test['mean_daily_TrAmt']

del temp
gc.collect()

temp=df.groupby('transaction_hour')['TransactionAmt'].agg('mean').to_frame().reset_index().rename({'TransactionAmt':'mean_hourly_TrAmt'}, axis=1)
train=train.merge(temp, on='transaction_hour', how='left')
test=test.merge(temp, on='transaction_hour', how='left')

train['diff_mean_hourly_TrAmt']=train['TransactionAmt']-train['mean_hourly_TrAmt']
test['diff_mean_hourly_TrAmt']=test['TransactionAmt']-test['mean_hourly_TrAmt']

del temp
gc.collect()

device_type_mean=df.groupby('DeviceType')['TransactionAmt'].mean().to_dict()
train['DeviceType_TrAmt_mean']=train['DeviceType'].map(device_type_mean)
test['DeviceType_TrAmt_mean']=test['DeviceType'].map(device_type_mean)
train['diff_DeviceType_TrAmt_mean']=train['TransactionAmt']-train['DeviceType_TrAmt_mean']
test['diff_DeviceType_TrAmt_mean']=test['TransactionAmt']-test['DeviceType_TrAmt_mean']
del device_type_mean
gc.collect()

productCD_mean=df.groupby('ProductCD')['TransactionAmt'].mean().to_dict()
train['ProductCD_TrAmt_mean']=train['ProductCD'].map(productCD_mean)
test['ProductCD_TrAmt_mean']=test['ProductCD'].map(productCD_mean)
train['diff_ProductCD_TrAmt_mean']=train['TransactionAmt']-train['ProductCD_TrAmt_mean']
test['diff_ProductCD_TrAmt_mean']=test['TransactionAmt']-test['ProductCD_TrAmt_mean']
del productCD_mean
gc.collect()

billing_location_mean=df.groupby('billing_location')['TransactionAmt'].mean().to_dict()
train['billing_location_TrAmt_mean']=train['billing_location'].map(billing_location_mean)
test['billing_location_TrAmt_mean']=test['billing_location'].map(billing_location_mean)
train['diff_billing_location_TrAmt_mean']=train['TransactionAmt']-train['billing_location_TrAmt_mean']
test['diff_billing_location_TrAmt_mean']=test['TransactionAmt']-test['billing_location_TrAmt_mean']
del billing_location_mean
gc.collect()

addr2_mean=df.groupby('addr2')['TransactionAmt'].mean().to_dict()
train['addr2_TrAmt_mean']=train['addr2'].map(addr2_mean)
test['addr2_TrAmt_mean']=test['addr2'].map(addr2_mean)
train['diff_addr2_TrAmt_mean']=train['TransactionAmt']-train['addr2_TrAmt_mean']
test['diff_addr2_TrAmt_mean']=test['TransactionAmt']-test['addr2_TrAmt_mean']
del addr2_mean
gc.collect()

card=['card1', 'card2','card3','card4','card5','card6']
for col in card:
  card_mean=df.groupby(col)['TransactionAmt'].mean().to_dict()
  train[col+'_TrAmt_mean']=train[col].map(card_mean)
  test[col+'_TrAmt_mean']=test[col].map(card_mean)
  train['diff'+col+'_TrAmt_mean']=train['TransactionAmt']-train[col+'_TrAmt_mean']
  test['diff'+col+'_TrAmt_mean']=test['TransactionAmt']-test[col+'_TrAmt_mean']
del card_mean
gc.collect()

timedelta_mean=df.groupby('D1')['TransactionAmt'].mean().to_dict()
train['timedelta_TrAmt_mean']=train['D1'].map(timedelta_mean)
test['timedelta_TrAmt_mean']=test['D1'].map(timedelta_mean)
train['diff_timedelta_TrAmt_mean']=train['TransactionAmt']-train['timedelta_TrAmt_mean']
test['diff_timedelta_TrAmt_mean']=test['TransactionAmt']-test['timedelta_TrAmt_mean']
del timedelta_mean
gc.collect()

timeZone_mean=df.groupby('id_14')['TransactionAmt'].mean().to_dict()
train['timeZone_TrAmt_mean']=train['id_14'].map(timeZone_mean)
test['timeZone_TrAmt_mean']=test['id_14'].map(timeZone_mean)
train['diff_timeZone_TrAmt_mean']=train['TransactionAmt']-train['timeZone_TrAmt_mean']
test['diff_timeZone_TrAmt_mean']=test['TransactionAmt']-test['timeZone_TrAmt_mean']
del timeZone_mean
gc.collect()

del df
gc.collect()

"""**Rolling Windows EWM**"""

window=[7,14,30]
df=pd.concat([train,test])
temp=df.groupby('transaction_day')['TransactionAmt'].agg('mean').to_frame().reset_index().rename({'TransactionAmt':'mean_daily_TrAmt'}, axis=1)

for day in window:
  temp['{}_days_EWM'.format(day)]=temp['mean_daily_TrAmt'].ewm(span=day, adjust=False).mean()
 

temp.drop('mean_daily_TrAmt', axis=1, inplace=True)

train=train.merge(temp, on='transaction_day', how='left')
test=test.merge(temp, on='transaction_day', how='left')

for day in window:
  train['diff_{}_day_EWM'.format(day)]=train['TransactionAmt']-train['{}_days_EWM'.format(day)]
  test['diff_{}_day_EWM'.format(day)]=test['TransactionAmt']-test['{}_days_EWM'.format(day)]

del temp,df
gc.collect()

"""**Columns' Stats**

Number of transactions for a card in a day:
"""

df=pd.concat([train,test])
temp=df.groupby(['transaction_day','card1'])['TransactionAmt'].count().to_frame().reset_index().rename({'TransactionAmt':'card1_tr_qty_daily'}, axis=1)

train=train.merge(temp, on=['transaction_day','card1'], how='left')
test=test.merge(temp, on=['transaction_day','card1'], how='left')

del temp,df

C=[]
for i in range(1,15):
  C.append('C{}'.format(i))

train['C_mean']=train[C].mean(axis=1)
train['C_sum']=train[C].sum(axis=1)
test['C_mean']=test[C].mean(axis=1)
test['C_sum']=test[C].sum(axis=1)

for i in range(1,15):
  train['TrAmt_C{}_ratio'.format(i)]=train['TransactionAmt']/train['C{}'.format(i)]
  test['TrAmt_C{}_ratio'.format(i)]=test['TransactionAmt']/test['C{}'.format(i)]

D=[]
for i in range(1,16):
  D.append('D{}'.format(i))

train['D_mean']=train[D].mean(axis=1)
train['D_sum']=train[D].sum(axis=1)
test['D_mean']=test[D].mean(axis=1)
test['D_sum']=test[D].sum(axis=1)

D.remove('D9')
for d in D:
  train['TrAmt_'+d+'_ratio']=train['TransactionAmt']/train[d]
  test['TrAmt_'+d+'_ratio']=test['TransactionAmt']/test[d]

V=[]
for i in range(1,340):
  V.append('V{}'.format(i))
  
train['V_mean']=train[V].mean(axis=1)
train['V_sum']=train[V].sum(axis=1)
test['V_mean']=test[V].mean(axis=1)
test['V_sum']=test[V].sum(axis=1)

ids_num=['id_01','id_02','id_03','id_04','id_05','id_06','id_07','id_08','id_09','id_10','id_11']

train['id_num_mean']=train[ids_num].mean(axis=1)
test['id_num_mean']=test[ids_num].mean(axis=1)

train.replace([np.inf, -np.inf], 0, inplace=True)
test.replace([np.inf, -np.inf], 0, inplace=True)

"""**Encoding Categorical Columns and Filling numeric NA's**"""

card=['card1', 'card2','card3','card4','card5','card6']
for col in card:
  train[col]=train[col].astype('object')
  test[col]=test[col].astype('object')

add=['addr1','addr2']
for col in add:
  train[col]=train[col].astype('object')
  test[col]=test[col].astype('object')

Categorical_features=[col for col in train.columns if train[col].dtype in ['category', 'object']]
Categorical_features

#encoding categorical/object columns
df=pd.concat([train,test])
for col in Categorical_features:
  df[col],_ = df[col].factorize()
  if df[col].max()<128: df[col] = df[col].astype('int8')
  elif df[col].max()<32768: df[col] = df[col].astype('int16')
  else: df[col].astype('int32')

train=df.iloc[:len(train)]
test=df.iloc[len(train):].drop('isFraud', axis=1).reset_index(drop=True)

del df
gc.collect()

# to fillna for the numerical values
train.fillna(-999, inplace=True)
test.fillna(-999, inplace=True)

for df in [train, test]:
  df.drop('TransactionID', axis=1, inplace=True)

len(train.columns)

X=train.drop('isFraud', axis=1)
y=train['isFraud']
X_test=test

from lightgbm import LGBMClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, GroupKFold

NFOLDS=3
#skf=StratifiedKFold(n_splits=NFOLDS, shuffle=True)
gkf=GroupKFold(n_splits=NFOLDS)
groups=np.ceil(train['transaction_day']/7)

clf = LGBMClassifier( 
    n_estimators= 2000,
    objective='binary',
    boosting_type= "gbdt",
    num_leaves= 1280,
    max_depth=-1,
    learning_rate= 0.1,
    reg_lambda= 50,
    colsample_bytree=0.5,
    importance_type='gain',
    metric='auc'
               )
feature_importances=np.zeros((len(X.columns),NFOLDS))
for j, (train_idx, test_idx) in enumerate(gkf.split(X,y, groups=groups)):
    fold = j + 1
    print('Fold:',fold)

    kf_X_train=X.iloc[train_idx]
    kf_y_train=y.iloc[train_idx]
    
    kf_X_test=X.iloc[test_idx]
    kf_y_test=y.iloc[test_idx]
    h = clf.fit(kf_X_train, kf_y_train, eval_set=[(kf_X_train,kf_y_train), (kf_X_test,kf_y_test)],
              eval_names=['train', 'test'], eval_metric='auc',verbose=50, early_stopping_rounds=300)
    feature_importances[:,j]=clf.feature_importances_
f_imp=np.mean(feature_importances, axis=1)

feature_imp = pd.DataFrame(sorted(zip(f_imp,X.columns)), columns=['Value','Feature'])
plt.figure(figsize=(20, 10))
sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False).iloc[:30])
plt.title('LGB Feature Importance')
plt.tight_layout()
plt.show()

plt.figure(figsize=(20, 10))
sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False).iloc[-50:])
plt.title('LGB Feature Importance')
plt.tight_layout()
plt.show()
del clf, h; x=gc.collect()

low_importance_features=feature_imp[feature_imp['Value']==0]['Feature'].to_list()
low_importance_features

train.drop(low_importance_features, axis=1, inplace=True)
test.drop(low_importance_features, axis=1, inplace=True)

len(train.columns)

train.to_csv('IEEE_train.csv', index=False)
test.to_csv('IEEE_test.csv', index=False)

from google.colab import files

files.download('IEEE_train.csv')
files.download('IEEE_test.csv')